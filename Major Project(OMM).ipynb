{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c388ba8-a7a9-4aef-b0aa-ac846eb69c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 0us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 1s/step - accuracy: 0.4899 - auc: 0.3204 - loss: 0.8717 - precision: 0.1678 - recall: 0.3185 - val_accuracy: 0.5000 - val_auc: 0.4861 - val_loss: 0.7429 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 1s/step - accuracy: 0.4993 - auc: 0.3222 - loss: 0.8289 - precision: 0.1736 - recall: 0.3180 - val_accuracy: 0.5000 - val_auc: 0.5049 - val_loss: 0.7550 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.5104 - auc: 0.3135 - loss: 0.8022 - precision: 0.1701 - recall: 0.3107 - val_accuracy: 0.5000 - val_auc: 0.5393 - val_loss: 0.7417 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 1s/step - accuracy: 0.5027 - auc: 0.3121 - loss: 0.7964 - precision: 0.1705 - recall: 0.3030 - val_accuracy: 0.5000 - val_auc: 0.5485 - val_loss: 0.7243 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.4768 - auc: 0.3037 - loss: 0.7969 - precision: 0.1668 - recall: 0.3036 - val_accuracy: 0.5010 - val_auc: 0.5335 - val_loss: 0.7161 - val_precision: 0.5005 - val_recall: 0.9993 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 1s/step - accuracy: 0.5073 - auc: 0.3209 - loss: 0.7673 - precision: 0.1757 - recall: 0.3081 - val_accuracy: 0.5003 - val_auc: 0.5413 - val_loss: 0.7112 - val_precision: 0.5002 - val_recall: 0.9980 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 1s/step - accuracy: 0.4795 - auc: 0.3164 - loss: 0.7874 - precision: 0.1675 - recall: 0.3061 - val_accuracy: 0.5007 - val_auc: 0.5361 - val_loss: 0.7076 - val_precision: 0.5003 - val_recall: 0.9980 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 1s/step - accuracy: 0.5051 - auc: 0.3104 - loss: 0.7586 - precision: 0.1696 - recall: 0.2934 - val_accuracy: 0.5010 - val_auc: 0.5333 - val_loss: 0.7064 - val_precision: 0.5005 - val_recall: 0.9993 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - accuracy: 0.4880 - auc: 0.3115 - loss: 0.7679 - precision: 0.1718 - recall: 0.3031 - val_accuracy: 0.5003 - val_auc: 0.5322 - val_loss: 0.7065 - val_precision: 0.5002 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 2s/step - accuracy: 0.4899 - auc: 0.3015 - loss: 0.7668 - precision: 0.1683 - recall: 0.2923 - val_accuracy: 0.5010 - val_auc: 0.5298 - val_loss: 0.7015 - val_precision: 0.5005 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 3s/step - accuracy: 0.4827 - auc: 0.3193 - loss: 0.7572 - precision: 0.1696 - recall: 0.3172 - val_accuracy: 0.5010 - val_auc: 0.5337 - val_loss: 0.7008 - val_precision: 0.5005 - val_recall: 0.9980 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.4824 - auc: 0.3141 - loss: 0.7538 - precision: 0.1683 - recall: 0.3177 - val_accuracy: 0.5013 - val_auc: 0.5310 - val_loss: 0.6998 - val_precision: 0.5007 - val_recall: 0.9993 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.4936 - auc: 0.3169 - loss: 0.7464 - precision: 0.1698 - recall: 0.3042 - val_accuracy: 0.5010 - val_auc: 0.5417 - val_loss: 0.6976 - val_precision: 0.5005 - val_recall: 0.9973 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 1s/step - accuracy: 0.4890 - auc: 0.3096 - loss: 0.7521 - precision: 0.1720 - recall: 0.3069 - val_accuracy: 0.5017 - val_auc: 0.5306 - val_loss: 0.6973 - val_precision: 0.5008 - val_recall: 0.9973 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 1s/step - accuracy: 0.4758 - auc: 0.3106 - loss: 0.7543 - precision: 0.1699 - recall: 0.3163 - val_accuracy: 0.5020 - val_auc: 0.5412 - val_loss: 0.6955 - val_precision: 0.5010 - val_recall: 0.9947 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m932s\u001b[0m 4s/step - accuracy: 0.4693 - auc: 0.2996 - loss: 0.7558 - precision: 0.1645 - recall: 0.3013 - val_accuracy: 0.5033 - val_auc: 0.5453 - val_loss: 0.6953 - val_precision: 0.5017 - val_recall: 0.9927 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 1s/step - accuracy: 0.4836 - auc: 0.3062 - loss: 0.7460 - precision: 0.1691 - recall: 0.2936 - val_accuracy: 0.5013 - val_auc: 0.5399 - val_loss: 0.6965 - val_precision: 0.5007 - val_recall: 0.9980 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 1s/step - accuracy: 0.4700 - auc: 0.3113 - loss: 0.7499 - precision: 0.1661 - recall: 0.3024 - val_accuracy: 0.5023 - val_auc: 0.5438 - val_loss: 0.6953 - val_precision: 0.5012 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.4848 - auc: 0.3140 - loss: 0.7446 - precision: 0.1737 - recall: 0.3252 - val_accuracy: 0.5013 - val_auc: 0.5404 - val_loss: 0.6952 - val_precision: 0.5007 - val_recall: 0.9993 - learning_rate: 2.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 0.4797 - auc: 0.3313 - loss: 0.7408 - precision: 0.1727 - recall: 0.3494 - val_accuracy: 0.5020 - val_auc: 0.5550 - val_loss: 0.6946 - val_precision: 0.5010 - val_recall: 0.9953 - learning_rate: 2.0000e-05\n",
      "Average Training Accuracy: 0.4973\n",
      "Average Training Loss: 0.7655\n",
      "Epoch 1/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 4s/step - accuracy: 0.4939 - auc: 0.3285 - loss: 0.7668 - precision: 0.1757 - recall: 0.3401 - val_accuracy: 0.5010 - val_auc: 0.4807 - val_loss: 0.7183 - val_precision: 0.5006 - val_recall: 0.8640 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m858s\u001b[0m 4s/step - accuracy: 0.4888 - auc: 0.3249 - loss: 0.7651 - precision: 0.1773 - recall: 0.3358 - val_accuracy: 0.5377 - val_auc: 0.5623 - val_loss: 0.6964 - val_precision: 0.5250 - val_recall: 0.7913 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m 74/219\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m35:44\u001b[0m 15s/step - accuracy: 0.4904 - auc: 0.0000e+00 - loss: 0.7736 - precision: 0.0000e+00 - recall: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, applications\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "train_dir = r\"D:\\Choudhury_Major\\rvf10k\\train\"\n",
    "valid_dir = r\"D:\\Choudhury_Major\\rvf10k\\valid\"\n",
    "\n",
    "# Parameters\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "autotune = tf.data.AUTOTUNE\n",
    "\n",
    "# Function to process images\n",
    "def process_image(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "    img = tf.image.random_saturation(img, 0.8, 1.2)\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_dataset(directory):\n",
    "    class_names = ['real', 'fake']\n",
    "    all_files, all_labels = [], []\n",
    "    for idx, name in enumerate(class_names):\n",
    "        class_path = os.path.join(directory, name)\n",
    "        for fname in os.listdir(class_path):\n",
    "            all_files.append(os.path.join(class_path, fname))\n",
    "            all_labels.append(idx)\n",
    "    files = tf.constant(all_files)\n",
    "    labels = tf.constant(all_labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    ds = ds.map(process_image, num_parallel_calls=autotune)\n",
    "    ds = ds.shuffle(1000).batch(batch_size).prefetch(autotune)\n",
    "    return ds, labels\n",
    "\n",
    "train_ds, train_labels = prepare_dataset(train_dir)\n",
    "val_ds, val_labels = prepare_dataset(valid_dir)\n",
    "\n",
    "# Compute class weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(train_labels.numpy()),\n",
    "                                                  y=train_labels.numpy())\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Load EfficientNetB4\n",
    "base_model = applications.EfficientNetB4(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Custom classifier with dropout and batchnorm\n",
    "inputs = layers.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20,\n",
    "                    callbacks=[early_stop, lr_scheduler], class_weight=class_weights)\n",
    "\n",
    "# Report training accuracy and loss\n",
    "avg_accuracy = np.mean(history.history['accuracy'])\n",
    "avg_loss = np.mean(history.history['loss'])\n",
    "print(f\"Average Training Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Fine-tune\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "finetune_history = model.fit(train_ds, validation_data=val_ds, epochs=10,\n",
    "                             callbacks=[early_stop, lr_scheduler], class_weight=class_weights)\n",
    "\n",
    "# Report fine-tuning accuracy and loss\n",
    "finetune_avg_accuracy = np.mean(finetune_history.history['accuracy'])\n",
    "finetune_avg_loss = np.mean(finetune_history.history['loss'])\n",
    "print(f\"Average Fine-tuning Accuracy: {finetune_avg_accuracy:.4f}\")\n",
    "print(f\"Average Fine-tuning Loss: {finetune_avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "val_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "val_preds = (model.predict(val_ds) > 0.5).astype(\"int32\")\n",
    "\n",
    "precision = precision_score(val_labels, val_preds)\n",
    "recall = recall_score(val_labels, val_preds)\n",
    "f1 = f1_score(val_labels, val_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"efficientnetb4_deepfake_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2caf13-bcda-4fa8-aa96-1eaa26de9eb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcrop_face\u001b[39m(image_np):\n\u001b[0;32m      4\u001b[0m     face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def crop_face(image_np):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        face = image_np[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        return face / 255.0\n",
    "    else:\n",
    "        return cv2.resize(image_np, (224, 224)) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a048819-cdb6-4c41-923a-fff7eefa5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (256, 256))\n",
    "    img = tf.numpy_function(crop_face, [img], tf.float32)\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d42be-1e70-47f5-a8d2-0aa1243c5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, applications\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Paths\n",
    "data_dir = r\"D:\\Choudhury_Major\\rvf10k\"\n",
    "\n",
    "# Parameters\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "autotune = tf.data.AUTOTUNE\n",
    "\n",
    "# Face cropping function\n",
    "def crop_face(image_np):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        face = image_np[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, img_size)\n",
    "        return face / 255.0\n",
    "    else:\n",
    "        return cv2.resize(image_np, img_size) / 255.0\n",
    "\n",
    "# Process image\n",
    "def process_image(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (256, 256))\n",
    "    img = tf.numpy_function(crop_face, [img], tf.float32)\n",
    "    return img, label\n",
    "\n",
    "# Prepare dataset\n",
    "file_paths, labels = [], []\n",
    "class_names = ['real', 'fake']\n",
    "for idx, name in enumerate(class_names):\n",
    "    class_path = os.path.join(data_dir, name)\n",
    "    for fname in os.listdir(class_path):\n",
    "        file_paths.append(os.path.join(class_path, fname))\n",
    "        labels.append(idx)\n",
    "\n",
    "train_files, val_files, train_labels_np, val_labels_np = train_test_split(file_paths, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels_np))\n",
    "train_ds = train_ds.map(process_image, num_parallel_calls=autotune).shuffle(1000).batch(batch_size).prefetch(autotune)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels_np))\n",
    "val_ds = val_ds.map(process_image, num_parallel_calls=autotune).batch(batch_size).prefetch(autotune)\n",
    "\n",
    "# Compute class weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(train_labels_np),\n",
    "                                                  y=train_labels_np)\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Load EfficientNetB4\n",
    "base_model = applications.EfficientNetB4(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Custom classifier with dropout and batchnorm\n",
    "inputs = layers.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=20,\n",
    "                    callbacks=[early_stop, lr_scheduler], class_weight=class_weights)\n",
    "\n",
    "# Report training accuracy and loss\n",
    "avg_accuracy = np.mean(history.history['accuracy'])\n",
    "avg_loss = np.mean(history.history['loss'])\n",
    "print(f\"Average Training Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Fine-tune\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "finetune_history = model.fit(train_ds, validation_data=val_ds, epochs=10,\n",
    "                             callbacks=[early_stop, lr_scheduler], class_weight=class_weights)\n",
    "\n",
    "# Report fine-tuning accuracy and loss\n",
    "finetune_avg_accuracy = np.mean(finetune_history.history['accuracy'])\n",
    "finetune_avg_loss = np.mean(finetune_history.history['loss'])\n",
    "print(f\"Average Fine-tuning Accuracy: {finetune_avg_accuracy:.4f}\")\n",
    "print(f\"Average Fine-tuning Loss: {finetune_avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "val_labels_tensor = tf.constant(val_labels_np)\n",
    "val_preds = (model.predict(val_ds) > 0.5).astype(\"int32\")\n",
    "\n",
    "precision = precision_score(val_labels_np, val_preds)\n",
    "recall = recall_score(val_labels_np, val_preds)\n",
    "f1 = f1_score(val_labels_np, val_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"efficientnetb4_deepfake_model.keras\")\n",
    "\n",
    "# Grad-CAM Visual Explanation\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_img_array(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (256, 256))\n",
    "    img = tf.numpy_function(crop_face, [img], tf.float32)\n",
    "    return tf.expand_dims(img, axis=0)\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        loss = predictions[:, 0]\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
    "    import matplotlib.cm as cm\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cm.jet(heatmap)[:, :, :3]\n",
    "    superimposed_img = heatmap * alpha + img / 255.0\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# test_img_path = \"/mnt/data/rvf10k/rvf10k/all/fake/example.jpg\"\n",
    "# img_array = get_img_array(test_img_path)\n",
    "# heatmap = make_gradcam_heatmap(img_array, model, \"top_conv\")  # name depends on EfficientNetB4's layer\n",
    "# display_gradcam(test_img_path, heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47017a4d-07c5-4ae8-80c5-9ad45ff51792",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05bb947b-bb4d-473a-b1cb-4eec13d1bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\priyanshu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d0f560-82de-4dad-9123-83686a7a79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9116eb-9bd5-4a8d-b64d-406463060d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset Loaded Successfully!\n",
      "Images shape: (10000, 128, 128, 3)\n",
      "Labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_data = []\n",
    "labels = []\n",
    "\n",
    "# Update this to your correct path\n",
    "base_path = r\"D:\\Choudhury_Major\\rvf10k\"\n",
    "sub_dirs = ['train', 'valid']\n",
    "categories = ['real', 'fake']\n",
    "img_size = 128\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    for category in categories:\n",
    "        folder_path = os.path.join(base_path, sub_dir, category)\n",
    "        label = categories.index(category)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"[ERROR] Folder not found: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(folder_path, img_file)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img = cv2.resize(img, (img_size, img_size))\n",
    "                    image_data.append(img)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "image_data = np.array(image_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"✅ Dataset Loaded Successfully!\")\n",
    "print(\"Images shape:\", image_data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f78e8c8-48b5-462d-b499-794721441ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset Loaded Successfully!\n",
      "X_train shape: (7000, 128, 128, 3)\n",
      "y_train shape: (7000,)\n",
      "X_valid shape: (3000, 128, 128, 3)\n",
      "y_valid shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(base_path, img_size=128):\n",
    "    data = []\n",
    "    labels = []\n",
    "    categories = ['real', 'fake']\n",
    "\n",
    "    for label, category in enumerate(categories):\n",
    "        folder = os.path.join(base_path, category)\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"[ERROR] Folder not found: {folder}\")\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(folder, file)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img = cv2.resize(img, (img_size, img_size))\n",
    "                    data.append(img)\n",
    "                    labels.append(label)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# 👇 Update this path according to your system\n",
    "base_path = \"D:/Choudhury_Major/rvf10k\"\n",
    "\n",
    "# Load train data\n",
    "X_train, y_train = load_dataset(os.path.join(base_path, \"train\"))\n",
    "\n",
    "# Load valid data\n",
    "X_valid, y_valid = load_dataset(os.path.join(base_path, \"valid\"))\n",
    "\n",
    "print(\"✅ Dataset Loaded Successfully!\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af9da55-6e17-45ef-82c0-4f662cf19474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split done!\n",
      "X_train_new shape: (8000, 128, 128, 3)\n",
      "X_val_new shape: (2000, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose X and y are from one combined folder\n",
    "# For example:\n",
    "X = np.concatenate([X_train, X_valid])\n",
    "y = np.concatenate([y_train, y_valid])\n",
    "\n",
    "# Split into new train and val sets\n",
    "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"✅ Split done!\")\n",
    "print(\"X_train_new shape:\", X_train_new.shape)\n",
    "print(\"X_val_new shape:\", X_val_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bfb4529-26ac-4c12-9c88-b7f6726b3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalized and generators created.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Normalize the pixel values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_valid = X_valid.astype('float32') / 255.0\n",
    "\n",
    "# Image Data Augmentation for training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# For validation set, no augmentation needed\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Flow the data using generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_valid, y_valid, batch_size=32)\n",
    "\n",
    "print(\"Data normalized and generators created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47375c22-37d1-429a-b1e4-1038535a8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = {i : weights[i] for i in range(len(weights))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2935e138-1d93-4e75-b03b-dd51ddaf05ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANSHU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D(2,2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49d9462-9be6-480e-b84f-52fd396333c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANSHU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.5251 - loss: 3.1060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 130ms/step - accuracy: 0.5251 - loss: 3.0996 - val_accuracy: 0.4987 - val_loss: 1.9954\n",
      "Epoch 2/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5515 - loss: 0.8102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 139ms/step - accuracy: 0.5516 - loss: 0.8102 - val_accuracy: 0.5223 - val_loss: 0.9471\n",
      "Epoch 3/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5642 - loss: 0.8215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 133ms/step - accuracy: 0.5642 - loss: 0.8214 - val_accuracy: 0.5670 - val_loss: 0.7554\n",
      "Epoch 4/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 140ms/step - accuracy: 0.5654 - loss: 0.7139 - val_accuracy: 0.5390 - val_loss: 0.8024\n",
      "Epoch 5/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5804 - loss: 0.7247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 139ms/step - accuracy: 0.5804 - loss: 0.7247 - val_accuracy: 0.5773 - val_loss: 0.6980\n",
      "Epoch 6/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5700 - loss: 0.7182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 139ms/step - accuracy: 0.5701 - loss: 0.7181 - val_accuracy: 0.5860 - val_loss: 0.6759\n",
      "Epoch 7/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 140ms/step - accuracy: 0.5854 - loss: 0.6908 - val_accuracy: 0.6083 - val_loss: 0.7006\n",
      "Epoch 8/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5726 - loss: 0.6786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 138ms/step - accuracy: 0.5725 - loss: 0.6786 - val_accuracy: 0.5823 - val_loss: 0.6712\n",
      "Epoch 9/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5707 - loss: 0.6731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 135ms/step - accuracy: 0.5708 - loss: 0.6731 - val_accuracy: 0.5997 - val_loss: 0.6620\n",
      "Epoch 10/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 135ms/step - accuracy: 0.5852 - loss: 0.6642 - val_accuracy: 0.6160 - val_loss: 0.6779\n",
      "Epoch 11/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 133ms/step - accuracy: 0.5787 - loss: 0.6713 - val_accuracy: 0.5723 - val_loss: 0.6997\n",
      "Epoch 12/15\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 133ms/step - accuracy: 0.5915 - loss: 0.6626 - val_accuracy: 0.5693 - val_loss: 0.7192\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "791897d6-c63f-466c-9d47-7bd7dda7d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "Accuracy: 0.5996666666666667\n",
      "Precision: 0.6027491408934708\n",
      "Recall: 0.5846666666666667\n",
      "F1 Score: 0.5935702199661591\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = (model.predict(X_valid) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
    "print(\"Precision:\", precision_score(y_valid, y_pred))\n",
    "print(\"Recall:\", recall_score(y_valid, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e631c82-9bed-4609-b206-8da420e28d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m1,792\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m590,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m590,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_conv4 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m590,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_conv4 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_conv4 (\u001b[38;5;33mConv2D\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m32,832\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,057,281</span> (76.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,057,281\u001b[0m (76.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,897</span> (128.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,897\u001b[0m (128.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> (76.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,024,384\u001b[0m (76.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load VGG19 base model without the top layer\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)  # binary classification\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f47f840-8243-4d4a-8a48-dcd1910edda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 2s/step - accuracy: 0.5122 - loss: 0.7134 - val_accuracy: 0.6197 - val_loss: 0.6673\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2339s\u001b[0m 11s/step - accuracy: 0.5461 - loss: 0.6906 - val_accuracy: 0.6383 - val_loss: 0.6526\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 2s/step - accuracy: 0.5741 - loss: 0.6726 - val_accuracy: 0.6387 - val_loss: 0.6440\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 9s/step - accuracy: 0.5924 - loss: 0.6642 - val_accuracy: 0.6463 - val_loss: 0.6359\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 2s/step - accuracy: 0.6108 - loss: 0.6560 - val_accuracy: 0.6537 - val_loss: 0.6305\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 2s/step - accuracy: 0.6133 - loss: 0.6534 - val_accuracy: 0.6600 - val_loss: 0.6285\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 2s/step - accuracy: 0.6303 - loss: 0.6485 - val_accuracy: 0.6630 - val_loss: 0.6223\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 2s/step - accuracy: 0.6434 - loss: 0.6435 - val_accuracy: 0.6643 - val_loss: 0.6263\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 2s/step - accuracy: 0.6315 - loss: 0.6478 - val_accuracy: 0.6643 - val_loss: 0.6201\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.6338 - loss: 0.6437 - val_accuracy: 0.6687 - val_loss: 0.6168\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 2s/step - accuracy: 0.6423 - loss: 0.6366 - val_accuracy: 0.6717 - val_loss: 0.6143\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 3s/step - accuracy: 0.6343 - loss: 0.6394 - val_accuracy: 0.6733 - val_loss: 0.6146\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 3s/step - accuracy: 0.6338 - loss: 0.6444 - val_accuracy: 0.6773 - val_loss: 0.6115\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 2s/step - accuracy: 0.6448 - loss: 0.6361 - val_accuracy: 0.6750 - val_loss: 0.6129\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 2s/step - accuracy: 0.6430 - loss: 0.6380 - val_accuracy: 0.6760 - val_loss: 0.6116\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 882ms/step - accuracy: 0.6395 - loss: 0.6359 - val_accuracy: 0.6810 - val_loss: 0.6083\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 812ms/step - accuracy: 0.6493 - loss: 0.6355 - val_accuracy: 0.6820 - val_loss: 0.6084\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 821ms/step - accuracy: 0.6347 - loss: 0.6380 - val_accuracy: 0.6793 - val_loss: 0.6057\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 827ms/step - accuracy: 0.6444 - loss: 0.6361 - val_accuracy: 0.6783 - val_loss: 0.6041\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 824ms/step - accuracy: 0.6414 - loss: 0.6331 - val_accuracy: 0.6857 - val_loss: 0.6042\n"
     ]
    }
   ],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "770bb39d-1a3b-450f-8326-5ae583d464f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X and y are your image data and labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    image_data, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c760d764-eb64-437a-944e-5b6d654e4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660385a1-e4a4-42ff-b89d-eb3a8de94101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANSHU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 517ms/step\n",
      "Accuracy: 0.6275\n",
      "Precision: 0.599687255668491\n",
      "Recall: 0.767\n",
      "F1 Score: 0.6731022378236069\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(val_generator) > 0.5).astype(\"int32\")\n",
    "y_true = y_val\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0abe6-aff9-4222-a6e7-37a220024345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
